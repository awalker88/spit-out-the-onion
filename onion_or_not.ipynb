{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Onion or Not\n",
    "\n",
    "The inspiration for this project was [Luke Feilberg](https://github.com/lukefeilberg/onion) data of reddit titles (headlines) from two subreddits: [r/TheOnion](https://www.reddit.com/r/TheOnion/) and [r/NotTheOnion](https://www.reddit.com/r/nottheonion/).\n",
    "r/TheOnion is a collection of the most popular articles from [The Onion](https://www.theonion.com/), a satirical news\n",
    "website. Submissions to r/NotTheOnion are instead articles from various news sources that, while legitimate, \"are so mind-blowingly ridiculous that you could have sworn they were from The Onion\". This presents an interesting\n",
    "classification problem: can an algorithm tell the difference between satirical headlines and headlines that reddit users\n",
    "think <i>could</i> be satirical.\n",
    "\n",
    "The [original dataset](https://github.com/lukefeilberg/onion/blob/master/OnionOrNot.csv) contained 24,000 records, but using the method Luke built for pulling data, I gathered an additional\n",
    "6,000 records to make the dataset balanced with 15,000 r/TheOnion headlines and 15,000 r/NotTheOnion headlines.\n",
    "\n",
    "Onion articles are often reposted on social media and misidentified by users as legitimate headlines, leading to confusion. Those confused users are said to have \"ate the onion\", so perhaps this model could be used help them spit it back out :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some examples of Onion headlines:\n",
    "[27-Year-Old Lies About Every Single Aspect Of His Life To Keep Parents From Worrying](https://local.theonion.com/27-year-old-lies-about-every-single-aspect-of-his-life-1819575793)\n",
    "[More People Should Have Donated: Wikipedia Has Announced They Will Be Forced To Take Down Their Entry For ‘Ostrich’ Due To Lack Of Funding](https://news.clickhole.com/more-people-should-have-donated-wikipedia-has-announce-1838637936)\n",
    "[CIA Realizes It's Been Using Black Highlighters All These Years](https://politics.theonion.com/cia-realizes-its-been-using-black-highlighters-all-thes-1819568147)\n",
    "\n",
    "Some examples of Not The Onion headlines:\n",
    "[UPS Loses Family's \\\\$846K Inheritance, Offers to Refund \\\\$32 Shipping Fee](https://www.abc.net.au/triplej/programs/hack/2020-edelman-trust-barometer-shows-growing-sense-of-inequality/11883788?fbclid=IwAR09iusXpbCQ6BM5Fmsk4MVBN3OWIk2L5E8UbQKFwjg6nWpLHKgMGP2UTfM)\n",
    "[Distilleries using high-proof alcohol to make hand sanitizer](https://abcnews.go.com/Business/wireStory/distilleries-high-proof-alcohol-make-hand-sanitizer-69632364)\n",
    "[People no longer believe working hard will lead to a better life, survey shows](https://www.abc.net.au/triplej/programs/hack/2020-edelman-trust-barometer-shows-growing-sense-of-inequality/11883788?fbclid=IwAR09iusXpbCQ6BM5Fmsk4MVBN3OWIk2L5E8UbQKFwjg6nWpLHKgMGP2UTfM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import transformers as hft\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textstat\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "pd.set_option('max_colwidth', 250)\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load in the saved data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pd.read_csv('OnionOrNot_large.csv')\n",
    "\n",
    "print(f'Number of r/TheOnion headlines: {len(data[data[\"label\"] == 1])}')\n",
    "print(f'Number of r/NotTheOnion headlines: {len(data[data[\"label\"] == 0])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some light Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# remove all headlines with length less than 15, as most below that limit seem to be nonsense/errors\n",
    "data = data[data['text'].str.len() >= 15]\n",
    "\n",
    "# several headlines are accidentally links, so remove headlines that start with 'https://'\n",
    "data = data[~data['text'].str.startswith('https://')]\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "batches = np.array_split(data, 30) # split into parts so we can batch load them through DistilBERT\n",
    "\n",
    "print(f'Number of r/TheOnion headlines after cleaning: {len(data[data[\"label\"] == 1])}')\n",
    "print(f'Number of r/NotTheOnion headlines after cleaning: {len(data[data[\"label\"] == 0])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Number of words in headline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def num_words(x):\n",
    "    return len(x.split(' '))\n",
    "\n",
    "data['num_words'] = data['text'].apply(num_words)\n",
    "\n",
    "print(f'Average number of words in r/TheOnion: {data[data[\"label\"] == 1][\"num_words\"].mean():.2f}')\n",
    "print(f'Average number of words in r/NotTheOnion: {data[data[\"label\"] == 0][\"num_words\"].mean():.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whether the headline contains the word 'onion'. Many of The Onion's headlines are something like: \"Welcome To Onion Social, The Onion's New Social Media Platform\", so this could be predictive."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def contains_onion(x):\n",
    "    if 'onion' in x.lower():\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "data['contains_onion'] = data['text'].apply(contains_onion)\n",
    "\n",
    "print(f'Number of r/TheOnion articles with \"onion\" in title: {data[(data[\"contains_onion\"] == 1) & (data[\"label\"] == 1)][\"contains_onion\"].count()}')\n",
    "print(f'Number of r/NotTheOnion articles with \"onion\" in title: {data[(data[\"contains_onion\"] == 1) & (data[\"label\"] == 0)][\"contains_onion\"].count()}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Calculates the grade level needed to read the title with [textstat](https://pypi.org/project/textstat/), which combines many metrics into one. Most of those metrics use things like number of syllables, characters per word, number of predefined \"easy\" words, etc."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# readability of headline (cap of 20, as it's supposed to correspond to grade level)\n",
    "def readability(x):\n",
    "    txt_stand = textstat.text_standard(x, float_output=True)\n",
    "    if txt_stand > 20:\n",
    "        return 20\n",
    "    else:\n",
    "        return txt_stand\n",
    "\n",
    "data['readability'] = data['text'].apply(readability)\n",
    "\n",
    "print(f'Average readability in r/TheOnion: {data[data[\"label\"] == 1][\"readability\"].mean():.2f}')\n",
    "print(f'Average readability in r/NotTheOnion: {data[data[\"label\"] == 0][\"readability\"].mean():.2f}')\n",
    "\n",
    "# plot distribution of readability\n",
    "onion_data = data[data['label'] == 1]\n",
    "not_the_onion_data = data[data['label'] == 0]\n",
    "\n",
    "sns.distplot(onion_data['readability'], bins=20, kde=False, label='Onion')\n",
    "sns.distplot(not_the_onion_data['readability'], bins=20, kde=False, label='r/NotTheOnion')\n",
    "plt.title('Readability')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Percent of words that are uppercase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def percent_uppercase(x):\n",
    "    num_words = x['num_words']\n",
    "    num_upper = 0\n",
    "    for word in x['text'].split(' '):\n",
    "        if len(word) > 0 and word[0].isupper() :\n",
    "            num_upper += 1\n",
    "\n",
    "    return num_upper / num_words\n",
    "\n",
    "data['percent_uppercase'] = data.apply(percent_uppercase, axis=1)\n",
    "\n",
    "print(f'Average percent of words that are capitalized in r/TheOnion articles: {100 * data[data[\"label\"] == 1][\"percent_uppercase\"].mean():.2f}%')\n",
    "print(f'Average percent of words that are capitalized in r/NotTheOnion articles: {100 * data[data[\"label\"] == 0][\"percent_uppercase\"].mean():.2f}%')\n",
    "\n",
    "\n",
    "onion_data = data[data['label'] == 1]\n",
    "not_the_onion_data = data[data['label'] == 0]\n",
    "\n",
    "# plot distribution of percent uppercase\n",
    "sns.distplot(onion_data['percent_uppercase'], bins=10, kde=False, label='r/TheOnion')\n",
    "sns.distplot(not_the_onion_data['percent_uppercase'], bins=10, kde=False, label='r/NotTheOnion')\n",
    "plt.title('Percent of words that are uppercase Onion Articles')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### percent_uppercase: too good of a predictor\n",
    "It would turn out that `percent_uppercase` is a pretty predictive feature. Most r/TheOnion headlines capitalize almost every word,\n",
    "while r/NotTheOnion is more inconsistent.\n",
    "\n",
    "I want to take a step back though and think about why that might be. Remember that these headlines are actually the\n",
    "<i>titles</i> of Reddit posts. r/NotTheOnion specifies that title of the post must be <b>exactly</b> the same as the\n",
    "news article's headline. r/TheOnion does not have any similar rules posted in their subreddit. Thus, this pattern could\n",
    "be affected by each subreddit's users or moderator behavior.\n",
    "\n",
    "I would also like our model closer to a general satire classifier than a \"r/NotTheOnion vs r/TheOnion\" classifier, so I\n",
    "remove `percent_uppercase` from our list of features as it will probably cause our model to be overfit to this\n",
    "particular dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = data.drop('percent_uppercase', axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# First Model\n",
    "I manually tried to classify 100 headlines and ended up with 81 correct. While not super scientific, I think that\n",
    "represents a decent benchmark for how we'd want a model to perform.\n",
    "\n",
    "Let's see how some easy-to-make models like logistic regression and a random forest classifier fair using the features\n",
    "we created."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split up data\n",
    "labels = data['label']\n",
    "features = data[['num_words', 'contains_onion', 'readability']]\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.25)\n",
    "\n",
    "# logistic regression\n",
    "first_lr_model = LogisticRegression()\n",
    "first_lr_model.fit(train_features, train_labels)\n",
    "print(f'LogisticRegression Train Accuracy: {first_lr_model.score(test_features, test_labels)}')\n",
    "\n",
    "# random forest\n",
    "first_rf_model = RandomForestClassifier()\n",
    "first_rf_model.fit(train_features, train_labels)\n",
    "print(f'RandomForestClassifier Train Accuracy: {first_rf_model.score(test_features, test_labels)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While these features and models are better than random chance, 62-65% accuracy is not very close to my \"human level\" accuracy of 81%. Luckily, there have been some great advancements in NLP in recent years that do a better job of taking into account the meanings of words in our headlines."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DistilBERT\n",
    "\n",
    "[BERT](https://github.com/google-research/bert) was a new NLP model released by Google in late 2018. It was a huge leap\n",
    "in the way we treat NLP, bringing together new ideas like attention and transformers in a way that helped get it some\n",
    "of the highest accuracy scores on many metrics.\n",
    "\n",
    "For our purpose of classifying Onion articles, we'll use Hugging Face's [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html),\n",
    "a version of BERT that preserves over 95% of BERT's accuracy while still being able to comfortably run on a laptop.\n",
    "From their package, we'll use two functions:\n",
    "\n",
    "\n",
    " - `db_tokenizer` is what will turn our text into a list of 'tokens' (eg.\"I didn't go to the supermarket\" turns into \\[\"I\", \"didn't\", \"go\", \"to\", \"the\", \"supermarket\"\\]).\n",
    "   - Note that it will continue to split the sentence up until it only contains tokens that are in the DistilBERT\n",
    "   dataset. For words it doesn't know, like the word \"counterpoint\", this is alright as it gets split into \"counter\" and \"point\". That looks reasonable. There are many words  this does not work for. \"Hulu\", for example, gets split into \"hu\" and \"lu\".\n",
    "\n",
    " - `db_model` is the DistilBERT model that we feed our tokens into to get our sentence representations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_tokenizer = hft.DistilBertTokenizer\n",
    "db_model = hft.DistilBertModel\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "db_tokenizer = db_tokenizer.from_pretrained('distilbert-base-uncased')\n",
    "db_model = db_model.from_pretrained('distilbert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize and encode the titles\n",
    "To feed our titles into DistilBERT, they first need to be [tokenized](https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html). The tokenizer also adds a \"[CLS]\" token at the beginning and a \"[SEP]\" token at the end, which the model would use to know how to separate sentences if our text contained multiple sentences.\n",
    "\n",
    "Of course, the DistilBERT model has to take in numbers as input, so we actually use `db_tokenizer.encode_plus()`,\n",
    "which gives us the index of each token instead of the actual text string. It also provides the attention mask,\n",
    "which we'll use later.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenized_batches = []\n",
    "for batch in tqdm(batches):\n",
    "    # get the length of the longest tokenized headline in this batch so we can pass that length to db_tokenizer.encode_plus()\n",
    "    batch['tokenized'] = batch['text'].apply(lambda x: db_tokenizer.tokenize(x, add_special_tokens=True))\n",
    "    max_tokens = batch['tokenized'].apply(len).max()\n",
    "\n",
    "    # store dictionary results of db_tokenizer.encode_plus() in a column, then split tokenized and attention_mask into their own columns\n",
    "    batch['sequence_dict'] = batch['tokenized'].apply(lambda x: db_tokenizer.encode_plus(x, add_special_tokens=True, max_length=max_tokens, pad_to_max_length=True))\n",
    "    batch['tokenized'] = batch['sequence_dict'].apply(lambda x: x['input_ids'])\n",
    "    batch['attention_mask'] = batch['sequence_dict'].apply(lambda x: x['attention_mask'])\n",
    "    tokenized_batches.append(batch.drop('sequence_dict', axis='columns'))\n",
    "\n",
    "# show an example encoded title\n",
    "print(f'Max tokenized length for first batch: {len(tokenized_batches[0].loc[0, \"tokenized\"])}')\n",
    "print(f'Ex. Tokenized/encoded version: {tokenized_batches[0].loc[0, \"tokenized\"]}')\n",
    "print(f'Ex. Original sentence: {db_tokenizer.decode(tokenized_batches[0].loc[0, \"tokenized\"])}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store tokenized batches as numpy arrays"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids_batches = []\n",
    "for batch in tokenized_batches:\n",
    "    input_ids_batches.append(np.array(batch['tokenized'].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Attention mask\n",
    "Our model shouldn't consider the \"[PAD]\" tokens when computing attention. When we run our model, we'll use the\n",
    "attention masks provided by `db_tokenizer.encode_plus()` to let the model know to ignore them. See details\n",
    "[here](https://huggingface.co/transformers/glossary.html#attention-mask)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "attention_masks = []\n",
    "for batch in tokenized_batches:\n",
    "    attention_masks.append(np.array(batch['attention_mask'].tolist()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Now for the fun part!\n",
    "This part puts each of our batches into the DistilBERT model, which will turn our array of headlines into arrays of\n",
    "768-d sentence embeddings. If these sentence embeddings contain useful information (which they have been shown to),\n",
    "then a downstream model should find them useful and should do a better job of classifying whether a headline is from r/TheOnion or r/NotTheOnion. (Note: this step takes about 10 minutes on my laptop)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "last_hidden_states = []\n",
    "\n",
    "for i in tqdm(range(len(input_ids_batches))):\n",
    "    # convert tokenized headlines(which you'll recall are actually the indexes of the tokens) array into a PyTorch tensor\n",
    "    input_ids = torch.tensor(input_ids_batches[i]).to(torch.int64)\n",
    "    # concert attention_mask array into a PyTorch tensor\n",
    "    attention_mask = torch.tensor(attention_masks[i])\n",
    "\n",
    "    # feed these tensors into the DistilBERT model\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states.append(db_model(input_ids, attention_mask=attention_mask))\n",
    "\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "By default, the model returns all of the weights for each encoding layer. We only want the last layer's weights to use as our word embeddings, however, so we select the last layer's weights and convert it to numpy with `last_hidden_state[0][:,0,:].numpy()`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "last_layer_features = []\n",
    "for last_hidden_state in last_hidden_states:\n",
    "    last_layer_features.append(last_hidden_state[0][:,0,:].numpy())\n",
    "\n",
    "unbatched_data = pd.DataFrame(np.concatenate(last_layer_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The final step before building the downstream model is to split into training and test sets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_data = data.join(unbatched_data)\n",
    "labels = db_data['label']\n",
    "features = db_data.drop('label', axis='columns')\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.25)\n",
    "\n",
    "# take out text as it's not a model feature, but save them so we can see which ones the model got right or wrong\n",
    "train_text, test_text = train_features.pop('text'), test_features.pop('text')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DistilBERT Results\n",
    "Using only an out-of-the-box Logistic Regression model, accuracy is <i>significantly</i> better, jumping up >20 percentage points to about 86%."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "db_lr_clf = LogisticRegression(max_iter=300)\n",
    "db_lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "print('lr train', db_lr_clf.score(train_features, train_labels))\n",
    "print('lr test', db_lr_clf.score(test_features, test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we plot a confusion matrix, we can see that the model has about the same accuracy for each class"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_pred, y_actual, title):\n",
    "    cm = confusion_matrix(list(y_actual), list(y_pred))\n",
    "    sns.heatmap(cm, linewidths=.5, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "probabilities = pd.DataFrame(db_lr_clf.predict_proba(test_features), columns=['prob_not_onion', 'prob_onion'])\n",
    "predictions = pd.concat([test_text.reset_index(drop=True), probabilities, test_labels.reset_index(drop=True), test_features.reset_index(drop=True)], axis=1)\n",
    "predictions['prediction'] = predictions['prob_not_onion'].apply(lambda x: 1 if x < 0.5 else 0)\n",
    "plot_confusion_matrix(predictions['prediction'], test_labels, 'lr_clf')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Where did things go wrong?\n",
    "85% isn't bad (it did better than me, after all), but if we want to improve, the first step would be to look at our mistakes.\n",
    "\n",
    "My first observation is that the r/Onion articles it misclassified are not funny"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "onion = predictions[predictions['label'] == 1].copy()\n",
    "not_onion = predictions[predictions['label'] == 0].copy()\n",
    "\n",
    "onion.sort_values(by='prob_not_onion', ascending=False)[['text', 'prob_not_onion', 'prediction', 'label']].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "not_onion.sort_values(by='prob_onion', ascending=False)[['text', 'prob_onion', 'prediction', 'label']].head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(train_features, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('xgb train', xgb_model.score(train_features, train_labels))\n",
    "print('xgb test', xgb_model.score(test_features, test_labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()\n",
    "exp = shap.TreeExplainer(xgb_model)\n",
    "shap_values = exp.shap_values(train_features)\n",
    "# shap.force_plot(exp.expected_value, shap_values[0,:], train_features.iloc[0,:])\n",
    "shap.summary_plot(shap_values, train_features, plot_type='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_exp = shap.KernelExplainer(db_lr_clf.predict_proba, train_features[:250])\n",
    "sample_lr = train_features[:250]\n",
    "lr_shap_values = lr_exp.shap_values(sample_lr, nsamples=100, l1_reg='aic')\n",
    "# shap.force_plot(exp.expected_value, shap_values[0,:], train_features.iloc[0,:])\n",
    "shap.summary_plot(shap_values, train_features, plot_type='bar')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network\n",
    "Neural networks are often good at finding patterns when there are many input variables. Here, however, it seems like it doesn't perform much better than Logistic Regression. With some hyperparameter tuning that could change, but I often favor simplicity over small gains in accuracy."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nn_model = Sequential()\n",
    "nn_model.add(Dense(units=128, activation='relu', kernel_regularizer=l2(0.001), input_dim=771))\n",
    "nn_model.add(Dense(units=128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "nn_model.add(Dense(units=128, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "nn_model.add(Dense(units=1, activation='sigmoid'))\n",
    "nn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "nn_model.fit(train_features, train_labels, epochs=10, batch_size=16,\n",
    "          validation_data=(test_features, test_labels), callbacks=[EarlyStopping(patience=3, monitor='val_loss')])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Enter your own title"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can enter your own title and the classifier will hopefully tell you how good you are at impersonating the Onion"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_onion(headline: str, dbm):\n",
    "    headline = headline.lower()\n",
    "    tknzd = db_tokenizer.tokenize(headline, add_special_tokens=True)\n",
    "    encd = np.array([db_tokenizer.encode(tknzd)])\n",
    "    inputs = torch.tensor(encd).to(torch.int64)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hid_state = db_model(inputs)\n",
    "        embed = last_hid_state[0][:,0,:].numpy()\n",
    "        df = pd.DataFrame(embed)\n",
    "        df = df * .0001\n",
    "        df.insert(0, 'num_words', num_words(headline))\n",
    "        df.insert(1, 'contains_onion', contains_onion(headline))\n",
    "        df.insert(2, 'readability', readability(headline))\n",
    "        pred = dbm.predict_proba(df)[0][0]\n",
    "        if pred < .5:\n",
    "            print(f'The classifier is {100*(1-pred):.1f}% sure this is The Onion')\n",
    "        else:\n",
    "            print(f'The classifier is {100*pred:.1f}% sure this is Not The Onion')\n",
    "\n",
    "predict_onion(\"Nation's dogs loving whatever's going on right now\", db_lr_clf)\n",
    "predict_onion(\"I Have Invented A Cute Animal Mascot Named ‘Genocide Camel’ If Any Corporation Would Like To Use Him As The Face Of Their Company\", db_lr_clf)\n",
    "predict_onion(\"Black Man shot by police after matching description for covid-19\", db_lr_clf)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}